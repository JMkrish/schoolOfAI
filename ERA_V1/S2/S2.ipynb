{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1tvR1RymMVyGbWwPOC3WqRTZ1DS3usRKH","timestamp":1687278895057}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wJptKBxALl-u","executionInfo":{"status":"ok","timestamp":1687633808907,"user_tz":-330,"elapsed":10867,"user":{"displayName":"Murali Krishna","userId":"15794814672619030511"}},"outputId":"e1875082-6f2e-43a7-ab3d-57d64ebbad7e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","!pip install torchsummary\n","from torchsummary import summary"]},{"cell_type":"code","source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","device"],"metadata":{"id":"00Owi1LBNY8L","executionInfo":{"status":"ok","timestamp":1687633808908,"user_tz":-330,"elapsed":12099,"user":{"displayName":"Murali Krishna","userId":"15794814672619030511"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["batch_size = 128\n","\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=True, download=True,\n","                    transform=transforms.Compose([\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.1307,), (0.3081,))\n","                    ])),\n","    batch_size=batch_size, shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.1307,), (0.3081,))\n","                    ])),\n","    batch_size=batch_size, shuffle=True)"],"metadata":{"id":"EQZaZRGcNLtr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Some Notes on our naive model\n","\n","We are going to write a network based on what we have learnt so far.\n","\n","The size of the input image is 28x28x1. We are going to add as many layers as required to reach RF = 32 \"atleast\"."],"metadata":{"id":"r3gEjf-xMb-N"}},{"cell_type":"code","source":["class FirstDNN(nn.Module):\n","  def __init__(self):\n","    super(FirstDNN, self).__init__()\n","    # r_in:1, n_in:28, j_in:1, s:1, r_out:3, n_out:28, j_out:1\n","    self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n","    # r_in:3 , n_in:28 , j_in:1 , s:1 , r_out:5 , n_out:28 , j_out:1\n","    self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n","    # r_in:5 , n_in:28 , j_in:1 , s:2 , r_out:6 , n_out:14 , j_out:2\n","    self.pool1 = nn.MaxPool2d(2, 2)\n","    # r_in:6 , n_in:14 , j_in:2 , s:1 , r_out: 10, n_out:14 , j_out:2\n","    self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n","    # r_in:10 , n_in:14 , j_in:2 , s: 1, r_out: 14, n_out: 14, j_out:2\n","    self.conv4 = nn.Conv2d(128, 256, 3, padding = 1)\n","    # r_in:14 , n_in: 14, j_in: 2, s: 2, r_out: 16, n_out: 7, j_out:4\n","    self.pool2 = nn.MaxPool2d(2, 2)\n","    # r_in: 16, n_in: 7, j_in: 4, s: 1, r_out: 24, n_out: 5, j_out:4\n","    self.conv5 = nn.Conv2d(256, 512, 3)\n","    # r_in:24 , n_in: 5, j_in: 4, s: 1, r_out: 32, n_out: 3, j_out:4\n","    self.conv6 = nn.Conv2d(512, 1024, 3)\n","    # r_in: 32, n_in: 3, j_in:4 , s: 1, r_out: 40, n_out: 1, j_out:4\n","    self.conv7 = nn.Conv2d(1024, 10, 3)\n","# Correct values\n","# https://user-images.githubusercontent.com/498461/238034116-7db4cec0-7738-42df-8b67-afa971428d39.png\n","  def forward(self, x):\n","    x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n","    x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n","    x = F.relu(self.conv6(F.relu(self.conv5(x))))\n","    x = self.conv7(x)\n","    x = x.view(-1, 10)\n","    return F.log_softmax(x)\n"],"metadata":{"id":"Sir2LmSVLr_4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = FirstDNN().to(device)"],"metadata":{"id":"sxICO4TTNt2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary(model, input_size=(1, 28, 28))"],"metadata":{"id":"M__MtFIYNwXa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    pbar = tqdm(train_loader)\n","    for batch_idx, (data, target) in enumerate(pbar):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"metadata":{"id":"g_vlC-bdNzo1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(1, 2):\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"metadata":{"id":"a0FYVWkGOFBS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(1, 2):\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"metadata":{"id":"reIBU667OG_c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6agTEkqzz6TZ"},"execution_count":null,"outputs":[]}]}